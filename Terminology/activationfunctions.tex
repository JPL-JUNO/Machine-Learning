\chapter{Activation Functions}
\section{Estimating class probabilities in multiclass classification via the softmax function}
The softmax function is a soft form of the argmax function; instead of giving a single class index, it provides the probability of each class. Therefore, it allows us to compute meaningful class probabilities in multiclass settings (multinomial logistic regression).

In softmax, the probability of a particular sample with net input $z$ belonging to the $i$th class can be computed with a normalization term in the denominator, that is, the sum of the exponentially weighted linear functions:
\begin{equation}
    p(z)=\sigma(z)=\frac{e^{z_i}}{\sum_{j=1}^{M}e^{z_j}}
\end{equation}
\section{Broadening the output spectrum using a hyperbolic tangent}
Another sigmoidal function that is often used in the hidden layers of artificial NNs is the hyperbolic tangent (commonly known as tanh), which can be interpreted as a rescaled version of the logistic function: