\chapter{Clustering}
The goal of clustering is to find a natural grouping in data so that items in the same cluster are more similar to each other than to those from different clusters.
\section{Prototype-based clustering}
Prototype-based clustering means that each cluster is represented by a prototype, which is usually either the \textbf{centroid} (average) of similar points with continuous features, or the \textbf{medoid} (the most representative or the point that minimizes the distance to all other points that belong to a particular cluster) in the case of categorical features.
\subsection{k-means clustering}
\begin{algorithm}
    \caption{The k-means algorithm\label{kmeans}}

    \Begin{
        Randomly pick $k$ centroids from the examples as initial cluster centers\;
        \Repeat{
            the cluster assignments do not change or a user-defined tolerance or maximum number of iterations is reached
        }{
            Assign each example to the nearest centroid, $\mu^{(i)}, j \in \{1,\dots, k\}$\;
            Move the centroids to the center of the examples that were assigned to it\;}
    }
\end{algorithm}
\subsection{k-means++}

\begin{algorithm}
    \caption{The k-means++ algorithm\label{kmeans++}}
    \Begin{
        Initialize an empty set, $M$, to store the $k$ centroids being selected\;
        Randomly choose the first centroid from the input examples and  $M \leftarrow \mu^{(j)}$\;
        \Repeat{k centroids are chosen}{
            For each example, $\bm{x}^{(i)}$, that is not in $M$, find the minimum squared distance, $d(x^{(i)}, M)^2$, to any of the centroids in $M$\;
            To randomly select the next centroid, $\mu^{(p)}$, from a weighted probability distribution equal to $\frac{d(\mu^{(p)},\textbf{M})^2}{\sum_id(x^{(i)},\textbf{M})^2}$\;
        }
        Proceed with the classic k-means algorithm\;
    }
\end{algorithm}
\subsection{Hard versus soft clustering}
\textbf{Hard clustering} describes a family of algorithms where each example in a dataset is assigned to exactly one cluster, as in the \autoref{kmeans} and \autoref{kmeans++}. In contrast, algorithms for \textbf{soft clustering} (sometimes also called \textbf{fuzzy clustering}) assign an example to one or more clusters. A popular example of soft clustering is the \textbf{fuzzy C-means (FCM)} algorithm (also called \textbf{soft k-means} or \textbf{fuzzy k-means}).
\subsection{Fuzzy C-means}
\begin{algorithm}
    \caption{The FCM algorithm\label{fcm}}
    \Begin{
        Specify the number of k centroids and randomly assign the cluster memberships for each point\;
        \Repeat{the membership coefficients do not change or a user-defined tolerance or maximum number of iterations is reached
        }{
            Compute the cluster centroids, $\bm{\mu}^{(i)}, j \in \{1,\dots, k\}$\;
            Update the cluster memberships for each point\;}
    }
\end{algorithm}

The objective function of FCMâ€”we abbreviate it as $J_m$:
\begin{equation}
    J_m=\sum_{i=1}^{n}\sum_{j=1}^{k}w^{(i,j)^m}||\bm{x}^{(i)}-\bm{\mu}^{(j)}||_2^2
\end{equation}

We added an additional exponent to $w^{(i,j)}$; the exponent $m$, any number greater than or equal to one (typically $m = 2$), is the so-called \textbf{fuzziness coefficient} (or simply \textbf{fuzzifier}), which controls the degree of fuzziness.

The larger the value of m, the smaller the cluster membership, $w^{(i,j)}$, becomes, which leads to fuzzier clusters. The cluster membership probability itself is calculated as follows:
\begin{equation}
    w^{(i,j)}=\left[\sum_{c=1}^{k}\left(\frac{||\bm{x}^{(i)}-\bm{\mu}^{(j)}||_2}{||\bm{x}^{(i)}-\bm{\mu}^{(c)}||_2}\right)^{\frac{2}{m-1}}\right]^{-1}
\end{equation}
The center, $\bm{\mu}^{(j)}$ , of a cluster itself is calculated as the mean of all examples weighted by the degree to
which each example belongs to that cluster ($w^{(i,j)^m}$):
\begin{equation}
    \bm{\mu}^{(j)}=\frac{\sum_{i=1}^{n}w^{(i,j)^m}\bm{x}^{(i)}}{\sum_{i=1}^{n}w^{(i,j)^m}}
\end{equation}