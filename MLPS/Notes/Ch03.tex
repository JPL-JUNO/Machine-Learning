\chapter{A Tour of Machine Learning Classifiers Using Scikit-Learn\label{Ch03}}
\section{Modeling class probabilities via logistic regression}
\subsection{Logistic regression and conditional probabilities}

Under the logistic model, we assume that there is a linear relationship between the weighted inputs and the log-odds:
\begin{equation}
    logit(p)=w_1x_1+w_2x_2+\cdots+w_mx_m+b=\sum_{i=j}w_jx_j=\textbf{w}^T\textbf{x}+b
\end{equation}

\figures{fig3-3}{Logistic regression compared to Adaline}
\subsection{Learning the model weights via the logistic loss function}
To explain how we can derive the loss function for logistic regression, let’s first define the likelihood, $\mathcal{L}$, that we want to maximize when we build a logistic regression model, assuming that the individual examples in our dataset are independent of one another. The formula is as follows:
\begin{equation}
    \begin{aligned}
        \mathcal{L}(\textbf{w}, b|\textbf{x}) & =p(y|\textbf{x};\textbf{w},b)=\prod_{i=1}^{m}p\left(y^{(i)}|\textbf{x}^{(i)};\textbf{w},b\right) \\
                                              & =\prod_{i=1}^{m}\left(\sigma(z^{(i)})\right)^{y^{(i)}}\left(1-\sigma(z^{(i)})\right)^{1-y^{(i)}}
    \end{aligned}
\end{equation}

In practice, it is easier to maximize the (natural) log of this equation, which is called the \textbf{log-likelihood} function:
\begin{equation}
    \begin{aligned}
        l(\textbf{w}, b|\textbf{x}) & =\log \mathcal{L}(\textbf{w}, b|\textbf{x})                                             \\
                                    & =\sum_{i=1}\left[y^{(i)}\log(\sigma(z^{(i)}))+(1-y^{(i)})\log(1-\sigma(z^{(i)}))\right]
    \end{aligned}
\end{equation}

\subsection{Tackling overfitting via regularization}
Overfitting is a common problem in machine learning, where a model performs well on training data but does not generalize well to unseen data (test data). If a model suffers from overfitting, we also say that the model has a high variance, which can be caused by having too many parameters, leading to a model that is too complex given the underlying data. Similarly, our model can also suffer from underfitting (high bias), which means that our model is not complex enough to capture the pattern in the training data well and therefore also suffers from low performance on unseen data.

\begin{tcolorbox}[title=The bias-variance tradeoff]
    Often, researchers use the terms “bias” and “variance” or “bias-variance tradeoff” to describe the performance of a model—that is, you may stumble upon talks, books, or articles where speople say that a model has a “high variance” or “high bias.” So, what does that mean? In general, we might say that “high variance” is proportional to overfitting and “high bias” is proportional to underfitting.

    In the context of machine learning models, variance measures the consistency (or variability) of the model prediction for classifying a particular example if we retrain the model multiple times, for example, on different subsets of the training dataset. We can say that the model is sensitive to the randomness in the training data. In contrast, bias measures how far off the predictions are from the correct values in general if we rebuild the model multiple times on different training datasets; bias is the measure of the systematic error that is not due to randomness.
\end{tcolorbox}

One way of finding a good bias-variance tradeoff is to tune the complexity of the model via regularization. Regularization is a very useful method for handling collinearity (high correlation among features), filtering out noise from data, and eventually preventing overfitting.

\begin{tcolorbox}[title=Regularization and feature normalization]
    Regularization is another reason why feature scaling such as standardization is important. For regularization to work properly, we need to ensure that all our features are on comparable scales.
\end{tcolorbox}

The loss function for logistic regression can be regularized by adding a simple regularization term, which will shrink the weights during model training:
\begin{equation}
    L(\textbf{w}, b|\textbf{x})  =\sum_{i=1}\left[y^{(i)}\log(\sigma(z^{(i)}))+(1-y^{(i)})\log(1-\sigma(z^{(i)}))\right]+\frac{\lambda}{2n}||\textbf{w}||^2
\end{equation}

The parameter, \textsf{C}, that is implemented for the LogisticRegression class in scikit-learn comes from a convention in support vector machines. The term \textsf{C} is inversely proportional to the regularization parameter, $\lambda$ . Consequently, decreasing the value of the inverse regularization parameter, \textsf{C}, means that we are increasing the regularization strength