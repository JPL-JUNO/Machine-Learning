\chapter{Compressing Data via Dimensionality Reduction\label{Ch05}}
\section{Unsupervised dimensionality reduction via principal component analysis}
The difference between feature selection and feature extraction is that while we maintain the original features when we use feature selection algorithms, such as sequential backward selection, we use feature extraction to transform or project the data onto a new feature space.

In the context of dimensionality reduction, feature extraction can be understood as an approach to data compression with the goal of maintaining most of the relevant information. In practice, feature extraction is not only used to improve storage space or the computational efficiency of the learning algorithm but can also improve the predictive performance by reducing the curse of dimensionality—especially if we are working with non-regularized models.
\subsection{The main steps in principal component analysis}
In a nutshell, PCA aims to find the directions of maximum variance in high-dimensional data and projects the data onto a new subspace with equal or fewer dimensions than the original one. The orthogonal axes (principal components) of the new subspace can be interpreted as the directions of maximum variance given the constraint that the new feature axes are orthogonal to each other, as illustrated in \autoref{fig5-1}:
\figures{fig5-1}{Using PCA to find the directions of maximum variance in a dataset}

If we use PCA for dimensionality reduction, we construct a $d\times k$-dimensional transformation matrix, \textbf{W}, that allows us to map a vector of the features of the training example, $\textbf{x}$, onto a new $k$-dimensional feature subspace that has fewer dimensions than the original $d$-dimensional feature space. For instance, the process is as follows. Suppose we have a feature vector, $\textbf{x}$:
$$\textbf{x}=[x_1,x_2, \cdots, x_d], \textbf{x}\in \mathbb{R}^d$$
which is then transformed by a transformation matrix, $\textbf{W}\in \mathbb{R}^{d\times k}$ :
$$\textbf{x}\textbf{W}=\textbf{z}$$
resulting in the output vector:
$$\textbf{z}=[z_1,z_2, \cdots, z_k], \textbf{z}\in \mathbb{R}^k$$

\begin{algorithm}
    \Begin{
        Standardize the $d$-dimensional dataset\;
        Construct the covariance matrix\;
        Decompose the covariance matrix into its eigenvectors and eigenvalues\;
        Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors;\
        Select $k$ eigenvectors, which correspond to the $k$ largest eigenvalues, where $k$ is the dimensionality of the new feature subspace ($k\leq d$)\;
        Construct a projection matrix, \textbf{W}, from the “top” $k$ eigenvectors\;
        Transform the $d$-dimensional input dataset, X, using the projection matrix, \textbf{W}, to obtain the new $k$-dimensional feature subspace\;
    }\caption{PCA algorithm for dimensionality reduction}
\end{algorithm}

\begin{tcolorbox}[title=Eigendecomposition: Decomposing a Matrix into Eigenvectors and Eigenvalues]

    Eigendecomposition, the factorization of a square matrix into so-called \textbf{eigenvalues} and \textbf{eigenvectors}, is at the core of the PCA procedure.

    The covariance matrix is a special case of a square matrix: it’s a symmetric matrix, which means that the matrix is equal to its transpose, $A = A^T$.

    When we decompose such a symmetric matrix, the eigenvalues are real (rather than complex) numbers, and the eigenvectors are orthogonal (perpendicular) to each other. Furthermore, eigenvalues and eigenvectors come in pairs. If we decompose a covariance matrix into its eigenvectors and eigenvalues, the eigenvectors associated with the highest eigenvalue corresponds to the direction of maximum variance in the dataset. Here, this “direction” is a linear transformation of the dataset’s feature columns.
\end{tcolorbox}
\subsection{Assessing feature contributions}
Sometimes, we are interested to know about how much each original feature contributes to a given principal component. These contributions are often called \textbf{loadings}.

The factor loadings can be computed by scaling the eigenvectors by the square root of the eigenvalues. The resulting values can then be interpreted as the correlation between the original features and the principal component.
\section{Supervised data compression via linear discriminant analysis}
The general concept behind LDA is very similar to PCA, but whereas PCA attempts to find the orthogonal component axes of maximum variance in a dataset, the goal in LDA is to find the feature subspace that optimizes class separability.

\subsection{Principal component analysis versus linear discriminant analysis}
One assumption in LDA is that the data is normally distributed. Also, we assume that the classes have identical covariance matrices and that the training examples are statistically independent of each other. However, even if one, or more, of those assumptions is (slightly) violated, LDA for dimensionality reduction can still work reasonably well.

\subsection{The inner workings of linear discriminant analysis}
\begin{algorithm}
    \Begin{

    }
\end{algorithm}